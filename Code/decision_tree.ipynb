{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRG5U56PDLb2"
      },
      "source": [
        "\n",
        "# Approach 1: Decision Tree\n",
        "\n",
        "## The idea:\n",
        "The input is target/topic and the statment and the output is for/against/neutral\n",
        "\n",
        "Data pre-processing: tokenizer to convert words into text \n",
        "+ topics into tokens \n",
        "+ emotion of statement to token\n",
        "\n",
        "There will be a model for each topic/target that will be put into the function. \n",
        "\n",
        "For targets that do not exist in the dataset:\n",
        "+ The first scenario is to look for close by topics and predict based on that\n",
        "+ The second scenario is to generate more data using OpenAI!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4qI6xSjBnFZ"
      },
      "source": [
        "## Installs and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECSoFkuVBpSq"
      },
      "outputs": [],
      "source": [
        "!pip install -q spacy\n",
        "!pip install -q scikit-learn\n",
        "!pp install -q gensim\n",
        "!pip install -q openai\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "svEoMcUd_UNR"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from scipy import spatial\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import pickle\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HKLtKBwjumz"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "#choose from multiple models https://github.com/RaRe-Technologies/gensim-data\n",
        "model = api.load(\"glove-twitter-50\")\n",
        "\n",
        "needed_words = ['against', 'no', 'nor', 'not', \"don't\", 'should', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "updated_stopwords = stopwords.words(\"english\")\n",
        "\n",
        "for word in needed_words:\n",
        "  if word in updated_stopwords:\n",
        "    updated_stopwords.remove(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GrTqaGiMi9I"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWxyntVSMQNb"
      },
      "source": [
        "## Code Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GYDxXB2HUFWp"
      },
      "outputs": [],
      "source": [
        "openai.api_key = \"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\"\n",
        "\n",
        "def create_new_data(num, sentiment, stance, topic):\n",
        "  \"\"\"\n",
        "  Generate tweets using openAI\n",
        "  :param num: Number of tweets. They dont always work\n",
        "  :param sentiment: have sentiment as a variable as well while generating tweets\n",
        "  :param stance: for or against the topic\n",
        "  :param topic: the topic for which you want to generate stance \n",
        "  \"\"\"\n",
        "  response = openai.Completion.create(\n",
        "    model=\"text-davinci-002\",\n",
        "    prompt=f\"make {num} {sentiment} tweets that are {stance} {topic}\", \n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        "  )\n",
        "\n",
        "  tweets = response['choices'][0]['text']\n",
        "  tweets = tweets.split(\"\\n\")\n",
        "  tweets = [tweet[3:] for tweet in tweets if len(tweet)>5 and tweet[0].isdigit()]\n",
        "\n",
        "  return tweets\n",
        "\n",
        "# print(create_new_data(5, \"sad\", \"in favor of\", \"battery\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LHgzSpxQE2ER"
      },
      "outputs": [],
      "source": [
        "def get_data(topic=\"Solar Power\"):\n",
        "  \"\"\"\n",
        "  Gets all the data for a particular input topic and returns a df\n",
        "  0 is against, 1 is pro, 2 is neither\n",
        "  \"\"\"\n",
        "  try:\n",
        "    filename = f'/content/drive/My Drive/University/CSE 573: Stance Detection /Datasets/Output/openai_gen/openai_{topic}.csv'\n",
        "    topic_df = pd.read_csv(filename)\n",
        "  except:\n",
        "    data_path = \"/content/drive/MyDrive/University/CSE 573: Stance Detection /Datasets/Final/data.csv\"\n",
        "    # data_path = \"/content/drive/MyDrive/University/CSE 573: Stance Detection /Datasets/Final/combined_original.csv\"\n",
        "    df = pd.read_csv(data_path)\n",
        "    topic_df = df.loc[df['Target'].isin([topic, topic.lower()])]\n",
        "\n",
        "    try:\n",
        "      all_topics = list(set(df['Target']))\n",
        "      all_topics = [each_topic.lower() for each_topic in all_topics if each_topic.lower() in model.vocab]\n",
        "      \n",
        "      # check if anything close to the topic exists in our current data\n",
        "      most_similar = model.wv.most_similar_to_given(topic.lower(), list(all_topics))\n",
        "      similarity_score = model.wv.similarity(topic.lower(), most_similar)\n",
        "    except:\n",
        "      similarity_score = 0\n",
        "\n",
        "    # Check if topic exists or not in our current data\n",
        "    if topic_df.empty:\n",
        "\n",
        "\n",
        "      if similarity_score<0.7:\n",
        "\n",
        "        # generate a new dataset\n",
        "        # Add fetching code logic here\n",
        "        query, target, stance = [], [], []\n",
        "\n",
        "        for emotion in [\"angry\", \"happy\", \"sad\", \"surprise\", \"sarcastic\", \"neutral\"]:\n",
        "          query = query + create_new_data(5, emotion, \"in favor of\", topic)\n",
        "          target = target + [topic for i in range(0, 5)]\n",
        "          stance = stance + [1 for i in range(0, 5)]\n",
        "        \n",
        "        for emotion in [\"angry\", \"happy\", \"sad\", \"surprise\", \"sarcastic\", \"neutral\"]:\n",
        "          query = query + create_new_data(5, emotion, \"against\", topic)\n",
        "          target = target + [topic for i in range(0, 5)]\n",
        "          stance = stance + [0 for i in range(0, 5)]\n",
        "\n",
        "        # print(len(query), len(target), len(stance))\n",
        "\n",
        "        for emotion in [\"angry\", \"happy\", \"sad\", \"surprise\", \"sarcastic\", \"neutral\"]:\n",
        "          query = query + create_new_data(5, emotion, \"neutral to\", topic)\n",
        "          target = target + [topic for i in range(0, 5)]\n",
        "          stance = stance + [2 for i in range(0, 5)]\n",
        "\n",
        "        # print(len(query), len(target), len(stance))\n",
        "\n",
        "        topic_df = pd.DataFrame(data={\"Query\":query, \"Target\":target, \"Stance\":stance})\n",
        "\n",
        "        filename = f'/content/drive/My Drive/University/CSE 573: Stance Detection /Datasets/Output/openai_gen/openai_{topic}.csv'\n",
        "        topic_df.to_csv(filename, index=False)\n",
        "      \n",
        "      else:\n",
        "        topic_df = df.loc[df['Target'].isin([most_similar, most_similar.lower()])]\n",
        "    \n",
        "  return topic_df\n",
        "\n",
        "# print(get_data(\"Battery\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G4sNFS5CIRDj"
      },
      "outputs": [],
      "source": [
        "def get_sentence_vector(row):\n",
        "  \"\"\"\n",
        "  Get the vectorised form of the sentence\n",
        "  Can add tfidf*word vect for more precision\n",
        "  \"\"\"\n",
        "\n",
        "  def preprocess(s):\n",
        "    \"\"\"\n",
        "    Preprocessing for sentence, converting to lower case and removing stop words\n",
        "    \"\"\"\n",
        "    s = word_tokenize(s)\n",
        "    s = [word.lower() for word in s if not word in updated_stopwords]\n",
        "    return s\n",
        "\n",
        "  vect = []\n",
        "  for word in preprocess(row[0]):\n",
        "    if word in model.vocab:\n",
        "      vect.append(model[word])\n",
        "  \n",
        "  # vect is a list of a list. each word a seperate vector\n",
        "  # final_vect will return a 1D np array, vector of the sentence\n",
        "  final_vect = np.mean(np.array(vect), axis=0)\n",
        "  \n",
        "  return final_vect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "GNm1DVNUXUyZ"
      },
      "outputs": [],
      "source": [
        "out_df_f1, out_df_acc, out_df_topic, out_df_dataset, out_df_model, out_df_pro_count, our_df_neutral_count, out_df_against_count = [], [], [], [], [], [], [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lXiQVCelPhCM"
      },
      "outputs": [],
      "source": [
        "def generate_cluster(topic=\"Solar Power\", model_type=\"decision tree\"):\n",
        "  \"\"\"\n",
        "  Generates a cluster for given topic\n",
        "  \"\"\"\n",
        "  topic_df = get_data(topic)\n",
        "\n",
        "  vectorized_data = []\n",
        "  for idx, row in topic_df.iterrows():\n",
        "    vectorized_data.append(get_sentence_vector(row))\n",
        "\n",
        "  print(topic_df)\n",
        "\n",
        "  X = vectorized_data\n",
        "  y = topic_df[\"Stance\"]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "  if model_type==\"kmeans\":\n",
        "    train_model = KMeans(n_clusters=3, random_state=42).fit(X=X_train)\n",
        "    y_pred = train_model.predict(X_test)\n",
        "\n",
        "    print(\"Most representative terms per cluster (based on centroids):\")\n",
        "    for i in range(3):\n",
        "        tokens_per_cluster = \"\"\n",
        "        most_representative = model.wv.most_similar(positive=[train_model.cluster_centers_[i]], topn=10)\n",
        "        for t in most_representative:\n",
        "            tokens_per_cluster += f\"{t[0]} \"\n",
        "        print(f\"Cluster {i}: {tokens_per_cluster}\")\n",
        "\n",
        "  elif model_type==\"decision tree\":\n",
        "    train_model = DecisionTreeClassifier(criterion = 'entropy', random_state = 42)\n",
        "    train_model.fit(X_train, y_train)\n",
        "    y_pred = train_model.predict(X_test)\n",
        "\n",
        "  # save the model to disk\n",
        "  filename = f'/content/drive/My Drive/University/CSE 573: Stance Detection /Datasets/Output/dt_models/backtranslated_{model_type}_{topic}_model.pkl'\n",
        "  pickle.dump(train_model, open(filename, 'wb'))\n",
        "\n",
        "  out_df_topic.append(topic)\n",
        "  out_df_f1.append(f1_score(y_test, y_pred, average='macro')*100)\n",
        "  out_df_acc.append(accuracy_score(y_test, y_pred))\n",
        "  out_df_dataset.append(\"Backtranslated\")\n",
        "  out_df_model.append(model_type)\n",
        "  try:\n",
        "    out_df_against_count.append(topic_df[\"Stance\"].value_counts()[0])\n",
        "  except:\n",
        "    out_df_against_count.append(0)\n",
        "  try:\n",
        "    out_df_pro_count.append(max(topic_df[\"Stance\"].value_counts()[1], 0))\n",
        "  except:\n",
        "    out_df_pro_count.append(0)\n",
        "  try:\n",
        "    our_df_neutral_count.append(max(topic_df[\"Stance\"].value_counts()[2], 0))\n",
        "  except:\n",
        "    our_df_neutral_count.append(0)\n",
        "\n",
        "  # print(\"-----------------------------------------\")\n",
        "  \n",
        "  # print(confusion_matrix(y_test, y_pred))\n",
        "  # print(\"Accuracy for {}: {:.2f}%\".format(topic, accuracy_score(y_test, y_pred)))\n",
        "  # print(\"F-1 score for {}: {:.2f}%\".format(topic, f1_score(y_test, y_pred, average='macro')*100))\n",
        "  # print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "  # print(\"-----------------------------------------\")\n",
        "  \n",
        "  return train_model\n",
        "\n",
        "# topic = \"Religion\"\n",
        "# generate_cluster(topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NAhS802FunBX"
      },
      "outputs": [],
      "source": [
        "def predict(topic, sentence):\n",
        "  \"\"\"\n",
        "  enter your topic and sentence and get output of what the model predicts!\n",
        "  \"\"\"\n",
        "\n",
        "  model_type = \"decision tree\"\n",
        "\n",
        "  try:\n",
        "    filename = f'/content/drive/My Drive/University/CSE 573: Stance Detection /Datasets/Output/dt_models/backtranslated_{model_type}_{topic}_model.pkl'\n",
        "    trained_model = pickle.load(open(filename, 'rb'))\n",
        "  except:\n",
        "    trained_model = generate_cluster(topic=topic, model_type=model_type)\n",
        "  \n",
        "  sentence = sentence.strip()\n",
        "  prediction = trained_model.predict([get_sentence_vector(sentence)])\n",
        "\n",
        "  return prediction\n",
        "\n",
        "# predict(topic=\"Atheism\", sentence=[\"God is not real\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "141caPpM3Ydz"
      },
      "outputs": [],
      "source": [
        "def predict_stance(topic, sentence):\n",
        "  out = predict(topic, sentence)[0]\n",
        "\n",
        "  if out == 0:\n",
        "    return \"Against\"\n",
        "  elif out == 1:\n",
        "    return \"Support\"\n",
        "  else:\n",
        "    return \"Neutral\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKh0QGdd_utJ"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/drive/MyDrive/University/CSE 573: Stance Detection /Datasets/Testing/FIXED_openai_generated_dataset_FIXED.csv\"\n",
        "real_df = pd.read_csv(data_path)\n",
        "\n",
        "out= []\n",
        "for idx, row in real_df.iterrows():\n",
        "  try:\n",
        "    pred = predict(row[1], row[0])[0]\n",
        "    out.append(pred)\n",
        "  except:\n",
        "    out.append(-1)\n",
        "\n",
        "new_df = real_df\n",
        "new_df[\"out\"] = out\n",
        "new_df.to_csv('/content/drive/My Drive/University/CSE 573: Stance Detection /Datasets/Output/decision_tree_out_real.csv', index=False)\n",
        "# for topic in set(df[\"Target\"]):\n",
        "#   # print(topic)\n",
        "#   predict(topic=topic, sentence=[\"God is not real\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "MoylWOx1ekba"
      },
      "outputs": [],
      "source": [
        "out_data = {\n",
        "    \"Target\": out_df_topic,\n",
        "    \"Model\": out_df_model,\n",
        "    \"Dataset\": out_df_dataset,\n",
        "    \"F1\": out_df_f1,\n",
        "    \"Accuracy\": out_df_acc,\n",
        "    \"Count of Pro\": out_df_pro_count,\n",
        "    \"Count of Neutral\": our_df_neutral_count,\n",
        "    \"Count of Against\": out_df_against_count\n",
        "}\n",
        "\n",
        "out_df = pd.DataFrame(data=out_data)\n",
        "out_df.to_csv('/content/drive/My Drive/University/CSE 573: Stance Detection /Datasets/Output/decision_tree_out.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MyMD9V9Oixtf",
        "outputId": "e0c5c995-d178-4c46-8b35-0e02418bbfd7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Support'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_stance(topic=\"Religion\", sentence=\"God is real\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkMRqFU14F_M"
      },
      "source": [
        "## References that are priceless\n",
        "1. https://stackoverflow.com/questions/29760935/how-to-get-vector-for-a-sentence-from-the-word2vec-of-tokens-in-sentence/\n",
        "2. https://dylancastillo.co/nlp-snippets-cluster-documents-using-word2vec/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
